{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDnJ3vHM3r86"
   },
   "source": [
    "# LSTM for Sentiment Analysis\n",
    "\n",
    "Sentiment analysis (a.k.a. opinion mining or emotion AI) is the use of natural language processing to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Flowchart \n",
    "\n",
    "To conduct sentiment analysis using LSTM, we need several processing steps: \n",
    "\n",
    "- convert the raw text-words into so-called tokens which are integer values. These tokens are really just indices into a list of the entire vocabulary. \n",
    "\n",
    "- convert these integer-tokens into so-called embeddings which are real-valued vectors, whose mapping will be trained along with the neural network, so as to map words with similar meanings to similar embedding-vectors. \n",
    "\n",
    "- input these embedding-vectors to a LSTM network which can take sequences of arbitrary length as input and output a kind of summary of what it has seen in the input. \n",
    "\n",
    "- use a sigmoid function to give us a value between 0.0 and 1.0, where 0.0 is taken to mean a negative sentiment and 1.0 means a positive sentiment. \n",
    "\n",
    "This whole process allows us to classify input-text as either having a negative or positive sentiment, and the flowchart is roughly:\n",
    "\n",
    "<img src='../figs/17_Flowchart.png' width = '400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehbsDmyx3r89"
   },
   "source": [
    "## IMDb Data Set\n",
    "\n",
    "In this notebook, we will go through a sentiment analysis on [IMDb movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) using Long-Short Term Memory (LSTM) network. IMDb (an acronym for Internet Movie Database) is an online database of information related to films, television series, home videos, video games, and streaming content online. There are 25,000 reviews with positive/negative sentiment labels in the training set and an equal amount in the test set. It comes with Keras by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DagmIX6t3r8-"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OdBPVfyS3r8_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-AiK88R3r9A"
   },
   "source": [
    "Import IMDB dataset using `imdb.load_data`:\n",
    "\n",
    "Words are ranked by how often they occur (in the training set) and only the `num_words` most frequent words are kept. Any less frequent word will appear as `oov_char` (out-of-vocabulary character) value in the sequence data. All words will be kept if `num_words` is not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GR0yE4If3r9A",
    "outputId": "01576bd8-3be9-44a2-f16c-086aa83917af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n",
      "17473536/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "vacab_size = 5000\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words = vacab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsOKW1163r9B"
   },
   "source": [
    "Confirm the dataset size to be 25000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gT6PJHlx3r9C",
    "outputId": "0c23e525-7592-4232-e02c-aa17e830bbc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3mM8IPS3r9D"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The features in `imdb` dataset are vector representations of **word indexes** for the reviews, not the raw texts themselves. That is, they are already tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDWTVBlG3r9E",
    "outputId": "8217e552-3474-4a00-dcc2-eeb77837a87a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 2,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 2,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 2,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 2,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVzUK8NR3r9E"
   },
   "source": [
    "We need to pad the sequence to the **same length** of a maximum of 500 words. For that Keras provides us with the `pad_sequences` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fsI4aDtO3r9E"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = 500)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJewpvHg3r9F"
   },
   "source": [
    "Let us visualize now how it has transformed our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4pO0vA03r9F",
    "outputId": "d296555f-884d-4031-be07-e01073a04d3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    1,   14,   22,   16,\n",
       "         43,  530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,\n",
       "        173,   36,  256,    5,   25,  100,   43,  838,  112,   50,  670,\n",
       "          2,    9,   35,  480,  284,    5,  150,    4,  172,  112,  167,\n",
       "          2,  336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,\n",
       "         13,  447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,\n",
       "         22,    4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,\n",
       "         43,  530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,\n",
       "         17,   12,   16,  626,   18,    2,    5,   62,  386,   12,    8,\n",
       "        316,    8,  106,    5,    4, 2223,    2,   16,  480,   66, 3785,\n",
       "         33,    4,  130,   12,   16,   38,  619,    5,   25,  124,   51,\n",
       "         36,  135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,\n",
       "         77,   52,    5,   14,  407,   16,   82,    2,    8,    4,  107,\n",
       "        117,    2,   15,  256,    4,    2,    7, 3766,    5,  723,   36,\n",
       "         71,   43,  530,  476,   26,  400,  317,   46,    7,    4,    2,\n",
       "       1029,   13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,\n",
       "         56,   26,  141,    6,  194,    2,   18,    4,  226,   22,   21,\n",
       "        134,  476,   26,  480,    5,  144,   30,    2,   18,   51,   36,\n",
       "         28,  224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,\n",
       "         88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,\n",
       "         16,    2,   19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m2_Neop3r9G"
   },
   "source": [
    "Now we need to build a `word_to_id` dictionary so that these indexes can be transformed into words for further analysis. In the dictionary, we will need to provide 'PAD' token to index 0, 'START' token to index 1, and 'UNK'' token to index 2. So we have to shift the default indexes by 3 to adjust these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8dfFFp1A83z"
   },
   "outputs": [],
   "source": [
    "imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nf_ZzoqD3r9G"
   },
   "outputs": [],
   "source": [
    "word_to_id = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PhRDMB23r9G"
   },
   "source": [
    "After building `word_to_it`, we need to build `id_to_word` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "iobTKTFt3r9H"
   },
   "outputs": [],
   "source": [
    "id_to_word = {idx:word for word, idx in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mL7gkx93r9H"
   },
   "source": [
    "Now we can provide `id_to_word` an index and it will output the word associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_mLICI9n3r9H",
    "outputId": "56476b91-cf09-4300-8cb8-08b5add897f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'movie'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kz_F78tc3r9H",
    "outputId": "4b1e9f67-5147-4d47-df75-acc6779994d9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NiN684w1u5qb",
    "outputId": "d42112a4-10fa-42d2-ceaa-e3428294f17b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Ojl3M_sBreNp",
    "outputId": "cc90e293-55e9-43bf-8676-7c19ddf99a23"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1987'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[5003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hilWQ38CsMJm",
    "outputId": "f608c1b2-e3e0-4e38-d83c-ca23c3b0d9cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"belmondo's\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5OyDYiT3r9I"
   },
   "source": [
    "Now we can read what we have in the training set. Less frequent words will appear as 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoR80Wku3r9I",
    "outputId": "21c9b451-09d2-4c22-e947-21cfb4fdc7d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(id_to_word[id] for id in x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGylitPT3r9I"
   },
   "source": [
    "The labels are either 0 (negative review) or 1 (positive review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RslVT5ZI3r9I",
    "outputId": "e27c95e3-e61c-444c-e128-2a60c76c2d19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOi9FSSg3r9J"
   },
   "source": [
    "## Build LSTM model\n",
    "\n",
    "We build a model that can input sequence of word indexes and output the probability of how good or bad the user review is.\n",
    "\n",
    "- Create the instance of sequential model\n",
    "\n",
    "- Add an **embedding layer** with maximum vocab size and dimension of output. It turns positive integers (word indexes) into dense vectors of fixed size. Embedding layer can only be used as the first layer in a model.\n",
    "\n",
    "- Add a layer of LSTM with many-to-one input-output style.\n",
    "\n",
    "- Add a Dense layer with sigmoid activation for predicting the probability.\n",
    "\n",
    "- Compile the model for training with loss function as binary cross-entropy, optimizer as adam, and metric as accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k89Mk_TkN5JF"
   },
   "source": [
    "#### Word Embedding\n",
    "\n",
    "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
    "\n",
    "It is an improvement over the more traditional bag-of-word model (e.g., TF-IDF) encoding schemes where large sparse vectors were used to represent each word or to score/weigh each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
    "\n",
    "Instead, in an embedding,\n",
    "\n",
    "- words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
    "\n",
    "- the position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "\n",
    "- the position of a word in the learned vector space is referred to as its embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7yCo4izN683"
   },
   "source": [
    "#### Keras Emedding Layer\n",
    "\n",
    "[Parameters](https://keras.io/api/layers/core_layers/embedding/#embedding):\n",
    "\n",
    "- `input_dim`: the size of vocabulary. It is the number of unique words in the vocabulary, which is 5000 in our case.\n",
    "\n",
    "- `output_dim`: the number of dimensions we wish to embed into. Each word will be represented by a vector of this much dimensions.\n",
    "\n",
    "- `input_length`: lenght of the maximum document, which is stored in `maxlen` variable in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "27sgdzAG3r9J"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import Sequential\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "def lstm_sentiment():\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(input_dim = vacab_size, output_dim = embedding_vector_length, input_length=500)) \n",
    "  model.add(LSTM(units = 100)) \n",
    "  model.add(Dense(units = 1, activation='sigmoid')) \n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4QJ0qD53r9K",
    "outputId": "ab19d7fa-208c-4f02-b1e7-3dfdccd52490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_sentiment()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfxgCRZP6SSD"
   },
   "source": [
    "We'll use the `ModelCheckpoint` callback to regularly save checkpoints, and\n",
    "the `EarlyStopping` callback to interrupt training when the validation loss\n",
    "is not improving for `patience=3` consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "a9W_CewQ6SB_"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = EarlyStopping(monitor=\"val_accuracy\", patience=3)\n",
    "\n",
    "modelckpt_callback = ModelCheckpoint(\n",
    "    monitor=\"val_accuracy\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQiWMuNH3r9K"
   },
   "source": [
    "### Training\n",
    "\n",
    "Now train the model by providing the training set, labels, epochs, batches, etc parameters. We will use `validation_data` with `x_test` and `y_test`.\n",
    "\n",
    "Batch size, epochs, LSTM units, etc. are all hyperparameters and can be tuned further for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKE-u-So3r9K",
    "outputId": "6d955460-cd0f-4942-a39a-f08b67b5735f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9641\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.86020, saving model to model_checkpoint.h5\n",
      "391/391 [==============================] - 40s 101ms/step - loss: 0.1017 - accuracy: 0.9641 - val_loss: 0.4719 - val_accuracy: 0.8602\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9542\n",
      "Epoch 00002: val_accuracy improved from 0.86020 to 0.86224, saving model to model_checkpoint.h5\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 0.1185 - accuracy: 0.9542 - val_loss: 0.4670 - val_accuracy: 0.8622\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9677\n",
      "Epoch 00003: val_accuracy improved from 0.86224 to 0.86232, saving model to model_checkpoint.h5\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 0.0935 - accuracy: 0.9677 - val_loss: 0.4856 - val_accuracy: 0.8623\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9710\n",
      "Epoch 00004: val_accuracy did not improve from 0.86232\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 0.0840 - accuracy: 0.9710 - val_loss: 0.5171 - val_accuracy: 0.8343\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9674\n",
      "Epoch 00005: val_accuracy did not improve from 0.86232\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 0.0948 - accuracy: 0.9674 - val_loss: 0.4708 - val_accuracy: 0.8521\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9808\n",
      "Epoch 00006: val_accuracy did not improve from 0.86232\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 0.0584 - accuracy: 0.9808 - val_loss: 0.5634 - val_accuracy: 0.8514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7890a9350>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64, callbacks=[es_callback, modelckpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWDE5UXI9hAM"
   },
   "source": [
    "Load the best validated model and check the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pP2vqarf8H53",
    "outputId": "f1a891e3-e511-4baf-a95f-74e318e5f17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 28s 36ms/step - loss: 0.4856 - accuracy: 0.8623\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(path_checkpoint)\n",
    "val_accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPaA9vgJ9wzv"
   },
   "source": [
    "### Prediction\n",
    "\n",
    "Let's predict some random reviews and see how our model performs. Note that our trained model can only deal an input of word indexes. We write a function for generating the prediction for a given review in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "owr1g27R9x9p"
   },
   "outputs": [],
   "source": [
    "# for user prediction\n",
    "def user_input_processing(review):\n",
    "    vec = []\n",
    "    for word in review.split(\" \"):\n",
    "        if word[-1] == \".\":\n",
    "            word = word[:-1]\n",
    "        vec.append(word_to_id[str.lower(word)])\n",
    "    vec_padded = sequence.pad_sequences([vec], 500)\n",
    "    print(review, model.predict(vec_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbOaMuNv927H"
   },
   "source": [
    "A good review example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2V6DU7k95XG",
    "outputId": "c3a89f2d-9984-41ec-a7ec-7f1267bdfb29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the finest films made in recent years. [[0.978964]]\n"
     ]
    }
   ],
   "source": [
    "user_input_processing(\"One of the finest films made in recent years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8FlOXZO-WnM"
   },
   "source": [
    "A bad review example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNZ0xDTq97DZ",
    "outputId": "ac38d4d1-db65-4478-f144-72252d166265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictable and bad. The acting was terrible and the story was common. [[0.00377825]]\n"
     ]
    }
   ],
   "source": [
    "user_input_processing(\"Predictable and bad. The acting was terrible and the story was common.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02_LSTM Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
