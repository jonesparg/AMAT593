{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "We use KNNs to create a model that directly predicts a class for a new data point based off of the features. We'll also introduce metric learning to improve the performance of KNN classifiers by constructing informed distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "Although KNNs use labelled data for inference, **they do not go through any training process because there are no model parameters to learn.** KNNs can be used for both classification (`KNeighborsClassifier`) and regression (`KNeighborsRegressor`) tasks.\n",
    "\n",
    "Given a labeled dataset $\\{(x_i, y_i)\\}_{i=1}^n$ and a sample $x$ to be predicted:\n",
    "\n",
    "1. Select a value $K$ and a **distance function**\n",
    "\n",
    "2. Compute the distance between $x$ and the $n$ training samples\n",
    "\n",
    "3. Take the $K$-nearest data samples, known as $K$-nearest neighbors\n",
    "\n",
    "4. - Classification: assign $x$ to the class that has the greatest number of its $K$-nearest neighbors\n",
    "   - Regression: average the target values of its $K$-nearest neighbors\n",
    "\n",
    "<img src='../figs/08_KNN.png' width = '300'>\n",
    "\n",
    "KNN's performance is influenced by \n",
    "\n",
    "* The **distance function** used to determine the nearest neighbors\n",
    "\n",
    "* **number of nearest neighbors** taken into account, i.e. value of $K$, used to classify  new samples\n",
    "\n",
    "\n",
    "In the case where variables have very different measurement scales, then one variable will have a much higher influence on the distance calculated than another, e.g., one variable is based on annual income in dollars, and the other is based on age in years. We need to **standardize** the data (training and testing) so that features are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "Set index_col=0 to use the first column as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>WTT</th>\n",
       "      <th>PTI</th>\n",
       "      <th>EQW</th>\n",
       "      <th>SBI</th>\n",
       "      <th>LQE</th>\n",
       "      <th>QWG</th>\n",
       "      <th>FDJ</th>\n",
       "      <th>PJF</th>\n",
       "      <th>HQE</th>\n",
       "      <th>NXJ</th>\n",
       "      <th>TARGET CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.913917</td>\n",
       "      <td>1.162073</td>\n",
       "      <td>0.567946</td>\n",
       "      <td>0.755464</td>\n",
       "      <td>0.780862</td>\n",
       "      <td>0.352608</td>\n",
       "      <td>0.759697</td>\n",
       "      <td>0.643798</td>\n",
       "      <td>0.879422</td>\n",
       "      <td>1.231409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.635632</td>\n",
       "      <td>1.003722</td>\n",
       "      <td>0.535342</td>\n",
       "      <td>0.825645</td>\n",
       "      <td>0.924109</td>\n",
       "      <td>0.648450</td>\n",
       "      <td>0.675334</td>\n",
       "      <td>1.013546</td>\n",
       "      <td>0.621552</td>\n",
       "      <td>1.492702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.721360</td>\n",
       "      <td>1.201493</td>\n",
       "      <td>0.921990</td>\n",
       "      <td>0.855595</td>\n",
       "      <td>1.526629</td>\n",
       "      <td>0.720781</td>\n",
       "      <td>1.626351</td>\n",
       "      <td>1.154483</td>\n",
       "      <td>0.957877</td>\n",
       "      <td>1.285597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.234204</td>\n",
       "      <td>1.386726</td>\n",
       "      <td>0.653046</td>\n",
       "      <td>0.825624</td>\n",
       "      <td>1.142504</td>\n",
       "      <td>0.875128</td>\n",
       "      <td>1.409708</td>\n",
       "      <td>1.380003</td>\n",
       "      <td>1.522692</td>\n",
       "      <td>1.153093</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.279491</td>\n",
       "      <td>0.949750</td>\n",
       "      <td>0.627280</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>1.232537</td>\n",
       "      <td>0.703727</td>\n",
       "      <td>1.115596</td>\n",
       "      <td>0.646691</td>\n",
       "      <td>1.463812</td>\n",
       "      <td>1.419167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>1.010953</td>\n",
       "      <td>1.034006</td>\n",
       "      <td>0.853116</td>\n",
       "      <td>0.622460</td>\n",
       "      <td>1.036610</td>\n",
       "      <td>0.586240</td>\n",
       "      <td>0.746811</td>\n",
       "      <td>0.319752</td>\n",
       "      <td>1.117340</td>\n",
       "      <td>1.348517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>0.575529</td>\n",
       "      <td>0.955786</td>\n",
       "      <td>0.941835</td>\n",
       "      <td>0.792882</td>\n",
       "      <td>1.414277</td>\n",
       "      <td>1.269540</td>\n",
       "      <td>1.055928</td>\n",
       "      <td>0.713193</td>\n",
       "      <td>0.958684</td>\n",
       "      <td>1.663489</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>1.135470</td>\n",
       "      <td>0.982462</td>\n",
       "      <td>0.781905</td>\n",
       "      <td>0.916738</td>\n",
       "      <td>0.901031</td>\n",
       "      <td>0.884738</td>\n",
       "      <td>0.386802</td>\n",
       "      <td>0.389584</td>\n",
       "      <td>0.919191</td>\n",
       "      <td>1.385504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1.084894</td>\n",
       "      <td>0.861769</td>\n",
       "      <td>0.407158</td>\n",
       "      <td>0.665696</td>\n",
       "      <td>1.608612</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.855806</td>\n",
       "      <td>1.061338</td>\n",
       "      <td>1.277456</td>\n",
       "      <td>1.188063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>0.837460</td>\n",
       "      <td>0.961184</td>\n",
       "      <td>0.417006</td>\n",
       "      <td>0.799784</td>\n",
       "      <td>0.934399</td>\n",
       "      <td>0.424762</td>\n",
       "      <td>0.778234</td>\n",
       "      <td>0.907962</td>\n",
       "      <td>1.257190</td>\n",
       "      <td>1.364837</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       WTT       PTI       EQW       SBI       LQE       QWG  \\\n",
       "0             0  0.913917  1.162073  0.567946  0.755464  0.780862  0.352608   \n",
       "1             1  0.635632  1.003722  0.535342  0.825645  0.924109  0.648450   \n",
       "2             2  0.721360  1.201493  0.921990  0.855595  1.526629  0.720781   \n",
       "3             3  1.234204  1.386726  0.653046  0.825624  1.142504  0.875128   \n",
       "4             4  1.279491  0.949750  0.627280  0.668976  1.232537  0.703727   \n",
       "..          ...       ...       ...       ...       ...       ...       ...   \n",
       "995         995  1.010953  1.034006  0.853116  0.622460  1.036610  0.586240   \n",
       "996         996  0.575529  0.955786  0.941835  0.792882  1.414277  1.269540   \n",
       "997         997  1.135470  0.982462  0.781905  0.916738  0.901031  0.884738   \n",
       "998         998  1.084894  0.861769  0.407158  0.665696  1.608612  0.943859   \n",
       "999         999  0.837460  0.961184  0.417006  0.799784  0.934399  0.424762   \n",
       "\n",
       "          FDJ       PJF       HQE       NXJ  TARGET CLASS  \n",
       "0    0.759697  0.643798  0.879422  1.231409             1  \n",
       "1    0.675334  1.013546  0.621552  1.492702             0  \n",
       "2    1.626351  1.154483  0.957877  1.285597             0  \n",
       "3    1.409708  1.380003  1.522692  1.153093             1  \n",
       "4    1.115596  0.646691  1.463812  1.419167             1  \n",
       "..        ...       ...       ...       ...           ...  \n",
       "995  0.746811  0.319752  1.117340  1.348517             1  \n",
       "996  1.055928  0.713193  0.958684  1.663489             0  \n",
       "997  0.386802  0.389584  0.919191  1.385504             1  \n",
       "998  0.855806  1.061338  1.277456  1.188063             1  \n",
       "999  0.778234  0.907962  1.257190  1.364837             1  \n",
       "\n",
       "[1000 rows x 12 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"KNN_Dataset\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"KNN_Dataset\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WTT</th>\n",
       "      <th>PTI</th>\n",
       "      <th>EQW</th>\n",
       "      <th>SBI</th>\n",
       "      <th>LQE</th>\n",
       "      <th>QWG</th>\n",
       "      <th>FDJ</th>\n",
       "      <th>PJF</th>\n",
       "      <th>HQE</th>\n",
       "      <th>NXJ</th>\n",
       "      <th>TARGET CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913917</td>\n",
       "      <td>1.162073</td>\n",
       "      <td>0.567946</td>\n",
       "      <td>0.755464</td>\n",
       "      <td>0.780862</td>\n",
       "      <td>0.352608</td>\n",
       "      <td>0.759697</td>\n",
       "      <td>0.643798</td>\n",
       "      <td>0.879422</td>\n",
       "      <td>1.231409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.635632</td>\n",
       "      <td>1.003722</td>\n",
       "      <td>0.535342</td>\n",
       "      <td>0.825645</td>\n",
       "      <td>0.924109</td>\n",
       "      <td>0.648450</td>\n",
       "      <td>0.675334</td>\n",
       "      <td>1.013546</td>\n",
       "      <td>0.621552</td>\n",
       "      <td>1.492702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.721360</td>\n",
       "      <td>1.201493</td>\n",
       "      <td>0.921990</td>\n",
       "      <td>0.855595</td>\n",
       "      <td>1.526629</td>\n",
       "      <td>0.720781</td>\n",
       "      <td>1.626351</td>\n",
       "      <td>1.154483</td>\n",
       "      <td>0.957877</td>\n",
       "      <td>1.285597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.234204</td>\n",
       "      <td>1.386726</td>\n",
       "      <td>0.653046</td>\n",
       "      <td>0.825624</td>\n",
       "      <td>1.142504</td>\n",
       "      <td>0.875128</td>\n",
       "      <td>1.409708</td>\n",
       "      <td>1.380003</td>\n",
       "      <td>1.522692</td>\n",
       "      <td>1.153093</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.279491</td>\n",
       "      <td>0.949750</td>\n",
       "      <td>0.627280</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>1.232537</td>\n",
       "      <td>0.703727</td>\n",
       "      <td>1.115596</td>\n",
       "      <td>0.646691</td>\n",
       "      <td>1.463812</td>\n",
       "      <td>1.419167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WTT       PTI       EQW       SBI       LQE       QWG       FDJ  \\\n",
       "0  0.913917  1.162073  0.567946  0.755464  0.780862  0.352608  0.759697   \n",
       "1  0.635632  1.003722  0.535342  0.825645  0.924109  0.648450  0.675334   \n",
       "2  0.721360  1.201493  0.921990  0.855595  1.526629  0.720781  1.626351   \n",
       "3  1.234204  1.386726  0.653046  0.825624  1.142504  0.875128  1.409708   \n",
       "4  1.279491  0.949750  0.627280  0.668976  1.232537  0.703727  1.115596   \n",
       "\n",
       "        PJF       HQE       NXJ  TARGET CLASS  \n",
       "0  0.643798  0.879422  1.231409             1  \n",
       "1  1.013546  0.621552  1.492702             0  \n",
       "2  1.154483  0.957877  1.285597             0  \n",
       "3  1.380003  1.522692  1.153093             1  \n",
       "4  0.646691  1.463812  1.419167             1  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Variables\n",
    "\n",
    "Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(df.drop('TARGET CLASS',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WTT</th>\n",
       "      <th>PTI</th>\n",
       "      <th>EQW</th>\n",
       "      <th>SBI</th>\n",
       "      <th>LQE</th>\n",
       "      <th>QWG</th>\n",
       "      <th>FDJ</th>\n",
       "      <th>PJF</th>\n",
       "      <th>HQE</th>\n",
       "      <th>NXJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123542</td>\n",
       "      <td>0.185907</td>\n",
       "      <td>-0.913431</td>\n",
       "      <td>0.319629</td>\n",
       "      <td>-1.033637</td>\n",
       "      <td>-2.308375</td>\n",
       "      <td>-0.798951</td>\n",
       "      <td>-1.482368</td>\n",
       "      <td>-0.949719</td>\n",
       "      <td>-0.643314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.084836</td>\n",
       "      <td>-0.430348</td>\n",
       "      <td>-1.025313</td>\n",
       "      <td>0.625388</td>\n",
       "      <td>-0.444847</td>\n",
       "      <td>-1.152706</td>\n",
       "      <td>-1.129797</td>\n",
       "      <td>-0.202240</td>\n",
       "      <td>-1.828051</td>\n",
       "      <td>0.636759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.788702</td>\n",
       "      <td>0.339318</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>0.755873</td>\n",
       "      <td>2.031693</td>\n",
       "      <td>-0.870156</td>\n",
       "      <td>2.599818</td>\n",
       "      <td>0.285707</td>\n",
       "      <td>-0.682494</td>\n",
       "      <td>-0.377850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982841</td>\n",
       "      <td>1.060193</td>\n",
       "      <td>-0.621399</td>\n",
       "      <td>0.625299</td>\n",
       "      <td>0.452820</td>\n",
       "      <td>-0.267220</td>\n",
       "      <td>1.750208</td>\n",
       "      <td>1.066491</td>\n",
       "      <td>1.241325</td>\n",
       "      <td>-1.026987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.139275</td>\n",
       "      <td>-0.640392</td>\n",
       "      <td>-0.709819</td>\n",
       "      <td>-0.057175</td>\n",
       "      <td>0.822886</td>\n",
       "      <td>-0.936773</td>\n",
       "      <td>0.596782</td>\n",
       "      <td>-1.472352</td>\n",
       "      <td>1.040772</td>\n",
       "      <td>0.276510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WTT       PTI       EQW       SBI       LQE       QWG       FDJ  \\\n",
       "0 -0.123542  0.185907 -0.913431  0.319629 -1.033637 -2.308375 -0.798951   \n",
       "1 -1.084836 -0.430348 -1.025313  0.625388 -0.444847 -1.152706 -1.129797   \n",
       "2 -0.788702  0.339318  0.301511  0.755873  2.031693 -0.870156  2.599818   \n",
       "3  0.982841  1.060193 -0.621399  0.625299  0.452820 -0.267220  1.750208   \n",
       "4  1.139275 -0.640392 -0.709819 -0.057175  0.822886 -0.936773  0.596782   \n",
       "\n",
       "        PJF       HQE       NXJ  \n",
       "0 -1.482368 -0.949719 -0.643314  \n",
       "1 -0.202240 -1.828051  0.636759  \n",
       "2  0.285707 -0.682494 -0.377850  \n",
       "3  1.066491  1.241325 -1.026987  \n",
       "4 -1.472352  1.040772  0.276510  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],\n",
    "                                                    test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KNN\n",
    "\n",
    "Remember that we are trying to come up with a model to predict whether someone will TARGET CLASS or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For KNN algorithm, there are two important factors: the number of nearest neighbors and the metric or norm used for measure the distance, i.e., $\\ell_p$ norm ($p\\geq1$)\n",
    "\n",
    "* Euclidean:  $\\|x-z\\|:=\\sqrt{\\sum_{i=1}^d  (x_i - z_i)^2 }$, $(p=2)$\n",
    "\n",
    "* Manhattan: $\\|x-z\\|_1:=\\sum_{i=1}^d  |x_i - z_i|$, $(p=1)$\n",
    "\n",
    "* Minkowski: $\\|x-z\\|_p := \\left(\\sum_{i=1}^d  (x_i - z_i)^p \\right)^{1/p}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1) # default: Euclidean distance, i.e., p=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "[[151   8]\n",
      " [ 15 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       159\n",
      "           1       0.94      0.89      0.92       141\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('K=1')\n",
    "# confusion matrix: [ [#TP, #FP], [#FN, #TN] ] \n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a K Value\n",
    "\n",
    "Do a for loop to pick a good K Value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test)) # misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error Rate')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the error rate tends to hover around 0.05-0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best K = 34\n",
      "[[153   6]\n",
      " [  7 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       159\n",
      "           1       0.96      0.95      0.95       141\n",
      "\n",
      "    accuracy                           0.96       300\n",
      "   macro avg       0.96      0.96      0.96       300\n",
      "weighted avg       0.96      0.96      0.96       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The best K = 34\n",
    "knn = KNeighborsClassifier(n_neighbors=34)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print('The best K = 34')\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Margin Nearest Neighbor\n",
    "\n",
    "[Large Margin Nearest Neighbor](https://en.wikipedia.org/wiki/Large_margin_nearest_neighbor) (LMNN) belongs to the class of metric learning. The goal of supervised metric learning algorithms is to transform points in a new space, in which \n",
    "\n",
    "* the distance between two points from the same class will be small\n",
    "\n",
    "* the distance between two points from different classes will be large.\n",
    "\n",
    "LMNN learns a **Mahalanobis distance** metric in the KNN classification setting. The learned metric attempts to keep close k-nearest neighbors from the same class, while keeping examples from different classes separated by a large margin. This algorithm makes no assumptions about the distribution of the data. **You'll need to first install the ``metric_learn`` library.**\n",
    "\n",
    "A **Mahalanobis distance** metric is induced by some matrix $\\mathbf{D}$:\n",
    "\n",
    "$$\n",
    "d_M(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{D}(\\mathbf{x} - \\mathbf{z})\\|\n",
    "$$\n",
    "\n",
    "<img src='../figs/08_Lmnn.png' width =  '600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance, i.e. $\\mathbf{D}$, is learned by solving the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{D}} \\sum_{i,j} \\eta_{i,j} \\|\\mathbf{D}(\\mathbf{x}_i - \\mathbf{x}_j)\\|^2 + c \\sum_{i,j,l} \\eta_{i,j} (1-y_{i,l})\\left[ 1+ \\|\\mathbf{D}(\\mathbf{x}_i - \\mathbf{x}_j)\\|^2 - \\|\\mathbf{D}(\\mathbf{x}_i - \\mathbf{x}_l)\\|^2\\right]_+\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is a data point, $\\mathbf{x}_j$ is one of its K-nearest neighbors sharing the same label, and $\\mathbf{x}_l$ is any instance within that region with different label, $\\eta_{i,j}, y_{i,j}\\in\\{0,1\\}$ are both the indicators, $\\eta_{i,j} = 1$ represents $\\mathbf{x}_j$ is the K-nearest neighbors (with same labels) of $\\mathbf{x}_i$, $y_{i,l}=0$ indicates $\\mathbf{x}_i$, $\\mathbf{x}_l$ belong to different classes, $[\\cdot]_+ = \\max(0,\\cdot)$ is the hinge loss.\n",
    "\n",
    "* The first optimization goal is achieved by minimizing the average distance between instances and their target neighbors of the same class.\n",
    "\n",
    "* The second goal is achieved by penalizing distances to $\\mathbf{x}_l$ that are less than one unit further away than to target neighbors $\\mathbf{x}_j$ (and therefore pushing them out of the local neighborhood of $\\mathbf{x}_i$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMNN(k=5, learn_rate=1e-06)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from metric_learn import LMNN \n",
    "\n",
    "lmnn = LMNN(k=5, learn_rate=1e-6)\n",
    "lmnn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function MahalanobisMixin.get_metric.<locals>.metric_fun at 0x7fd9ea9c10d0>\n"
     ]
    }
   ],
   "source": [
    "print(lmnn.get_metric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=5\n",
      "[[154   5]\n",
      " [ 11 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       159\n",
      "           1       0.96      0.92      0.94       141\n",
      "\n",
      "    accuracy                           0.95       300\n",
      "   macro avg       0.95      0.95      0.95       300\n",
      "weighted avg       0.95      0.95      0.95       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric=lmnn.get_metric())\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print('K=5')\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [132]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mi, metric\u001b[38;5;241m=\u001b[39mlmnn\u001b[38;5;241m.\u001b[39mget_metric())\n\u001b[1;32m      7\u001b[0m     knn\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n\u001b[0;32m----> 8\u001b[0m     pred_i \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     error_rate\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(pred_i \u001b[38;5;241m!=\u001b[39m y_test)) \u001b[38;5;66;03m# misclassification rate\u001b[39;00m\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:214\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m        Class labels for each data sample.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     neigh_dist, neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    216\u001b[0m     _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py:776\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m         parallel_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 776\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparallel_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_tree_query_parallel_helper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal: _fit_method not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py:600\u001b[0m, in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tree_query_parallel_helper\u001b[39m(tree, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;124;03m\"\"\"Helper for the Parallel calls in KNeighborsMixin.kneighbors.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m    The Cython method tree.query is not directly picklable by cloudpickle\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m    under PyPy.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:1295\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree.query\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:1753\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:1753\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:1755\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:1755\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:1736\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/neighbors/_binary_tree.pxi:1143\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree.rdist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/metrics/_dist_metrics.pyx:312\u001b[0m, in \u001b[0;36msklearn.metrics._dist_metrics.DistanceMetric.rdist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/metrics/_dist_metrics.pyx:1133\u001b[0m, in \u001b[0;36msklearn.metrics._dist_metrics.PyFuncDistance.dist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/metrics/_dist_metrics.pyx:1141\u001b[0m, in \u001b[0;36msklearn.metrics._dist_metrics.PyFuncDistance._dist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/metric_learn/base_metric.py:280\u001b[0m, in \u001b[0;36mMahalanobisMixin.get_metric.<locals>.metric_fun\u001b[0;34m(u, v, squared)\u001b[0m\n\u001b[1;32m    278\u001b[0m dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(transformed_diff, transformed_diff\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m--> 280\u001b[0m   dist \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,40):\n",
    "    lmnn = LMNN(k=i, learn_rate=1e-6)\n",
    "    lmnn.fit(X_train, y_train)\n",
    "    knn = KNeighborsClassifier(n_neighbors=i, metric=lmnn.get_metric())\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test)) # misclassification rate\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with data-driven metric learning outperforms the original KNN with standard metrics at different levels of K value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Age of Abalone\n",
    "\n",
    "The [Abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone) contains age measurements on a large number of abalones. The age of an [abalone](https://en.wikipedia.org/wiki/Abalone) can be found by cutting its shell and counting the number of rings on the shell. \n",
    "\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "Given is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: \n",
    "\n",
    "- Sex: nominal / -- / M, F, and I (infant) \n",
    "- Length: continuous / mm / Longest shell measurement \n",
    "- Diameter: continuous / mm / perpendicular to length \n",
    "- Height: continuous / mm / with meat in shell \n",
    "- Whole weight: continuous / grams / whole abalone \n",
    "- Shucked weight: continuous / grams / weight of meat \n",
    "- Viscera weight: continuous / grams / gut weight (after bleeding) \n",
    "- Shell weight: continuous / grams / after being dried \n",
    "- Rings: integer / -- / +1.5 gives the age in years \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "abalone = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3       4       5       6      7   8\n",
       "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
       "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
       "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
       "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
       "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.columns = [\n",
    "    \"Sex\",\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole weight\",\n",
    "    \"Shucked weight\",\n",
    "    \"Viscera weight\",\n",
    "    \"Shell weight\",\n",
    "    \"Rings\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
       "0      M   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1      M   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2      F   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3      M   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4      I   0.330     0.255   0.080        0.2050          0.0895   \n",
       "...   ..     ...       ...     ...           ...             ...   \n",
       "4172   F   0.565     0.450   0.165        0.8870          0.3700   \n",
       "4173   M   0.590     0.440   0.135        0.9660          0.4390   \n",
       "4174   M   0.600     0.475   0.205        1.1760          0.5255   \n",
       "4175   F   0.625     0.485   0.150        1.0945          0.5310   \n",
       "4176   M   0.710     0.555   0.195        1.9485          0.9455   \n",
       "\n",
       "      Viscera weight  Shell weight  Rings  \n",
       "0             0.1010        0.1500     15  \n",
       "1             0.0485        0.0700      7  \n",
       "2             0.1415        0.2100      9  \n",
       "3             0.1140        0.1550     10  \n",
       "4             0.0395        0.0550      7  \n",
       "...              ...           ...    ...  \n",
       "4172          0.2390        0.2490     11  \n",
       "4173          0.2145        0.2605     10  \n",
       "4174          0.2875        0.3080      9  \n",
       "4175          0.2610        0.2960     10  \n",
       "4176          0.3765        0.4950     12  \n",
       "\n",
       "[4177 rows x 9 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sex is not a purely physical measure, we remove it from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.drop(\"Sex\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is Rings. A histogram will give a quick and useful overview of the age ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "abalone[\"Rings\"].hist(bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abalone.drop(\"Rings\", axis=1)\n",
    "y = abalone[\"Rings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``MinMaxScalar`` to normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X)\n",
    "X_scaler = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaler, y, test_size=0.2, random_state=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=3)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  2.3861364161083864\n"
     ]
    }
   ],
   "source": [
    "test_preds = knn_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "print('Test RMSE: ', sqrt(test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the performance using cross validation ``GridSearchCV`` to select hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 98 candidates, totalling 490 fits\n",
      "[CV 1/5] END ................n_neighbors=1, p=1;, score=0.188 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=1, p=1;, score=0.148 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=1, p=1;, score=0.135 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=1, p=1;, score=0.164 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=1, p=1;, score=0.215 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=1, p=2;, score=0.122 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=1, p=2;, score=0.189 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=1, p=2;, score=0.184 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=1, p=2;, score=0.195 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=1, p=2;, score=0.241 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=2, p=1;, score=0.341 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=2, p=1;, score=0.394 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=2, p=1;, score=0.374 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=2, p=1;, score=0.371 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=2, p=1;, score=0.393 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=2, p=2;, score=0.319 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=2, p=2;, score=0.399 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=2, p=2;, score=0.384 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=2, p=2;, score=0.367 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=2, p=2;, score=0.391 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=3, p=1;, score=0.383 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=3, p=1;, score=0.435 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=3, p=1;, score=0.470 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=3, p=1;, score=0.453 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=3, p=1;, score=0.427 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=3, p=2;, score=0.369 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=3, p=2;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=3, p=2;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=3, p=2;, score=0.438 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=3, p=2;, score=0.455 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=4, p=1;, score=0.420 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=4, p=1;, score=0.458 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=4, p=1;, score=0.485 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=4, p=1;, score=0.470 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=4, p=1;, score=0.462 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=4, p=2;, score=0.406 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=4, p=2;, score=0.487 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=4, p=2;, score=0.486 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=4, p=2;, score=0.477 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=4, p=2;, score=0.490 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=5, p=1;, score=0.436 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=5, p=1;, score=0.477 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=5, p=1;, score=0.497 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=5, p=1;, score=0.507 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=5, p=1;, score=0.486 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=5, p=2;, score=0.435 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=5, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=5, p=2;, score=0.513 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=5, p=2;, score=0.495 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=5, p=2;, score=0.499 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=6, p=1;, score=0.458 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=6, p=1;, score=0.491 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=6, p=1;, score=0.504 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=6, p=1;, score=0.524 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=6, p=1;, score=0.502 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=6, p=2;, score=0.466 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=6, p=2;, score=0.510 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=6, p=2;, score=0.526 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=6, p=2;, score=0.516 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=6, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=7, p=1;, score=0.466 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=7, p=1;, score=0.495 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=7, p=1;, score=0.515 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=7, p=1;, score=0.537 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=7, p=1;, score=0.512 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=7, p=2;, score=0.488 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=7, p=2;, score=0.516 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=7, p=2;, score=0.528 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=7, p=2;, score=0.526 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=7, p=2;, score=0.537 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=8, p=1;, score=0.477 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=8, p=1;, score=0.507 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=8, p=1;, score=0.510 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=8, p=1;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=8, p=1;, score=0.520 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=8, p=2;, score=0.488 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=8, p=2;, score=0.517 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=8, p=2;, score=0.531 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=8, p=2;, score=0.546 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=8, p=2;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=9, p=1;, score=0.489 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=9, p=1;, score=0.515 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=9, p=1;, score=0.513 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=9, p=1;, score=0.537 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=9, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 1/5] END ................n_neighbors=9, p=2;, score=0.494 total time=   0.0s\n",
      "[CV 2/5] END ................n_neighbors=9, p=2;, score=0.516 total time=   0.0s\n",
      "[CV 3/5] END ................n_neighbors=9, p=2;, score=0.537 total time=   0.0s\n",
      "[CV 4/5] END ................n_neighbors=9, p=2;, score=0.540 total time=   0.0s\n",
      "[CV 5/5] END ................n_neighbors=9, p=2;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=10, p=1;, score=0.497 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=10, p=1;, score=0.516 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=10, p=1;, score=0.517 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=10, p=1;, score=0.538 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=10, p=1;, score=0.537 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=10, p=2;, score=0.494 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=10, p=2;, score=0.519 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=10, p=2;, score=0.535 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=10, p=2;, score=0.540 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=10, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=11, p=1;, score=0.499 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=11, p=1;, score=0.522 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=11, p=1;, score=0.524 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=11, p=1;, score=0.539 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=11, p=1;, score=0.539 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=11, p=2;, score=0.495 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=11, p=2;, score=0.525 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=11, p=2;, score=0.533 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=11, p=2;, score=0.542 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=11, p=2;, score=0.547 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=12, p=1;, score=0.505 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=12, p=1;, score=0.519 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=12, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=12, p=1;, score=0.542 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=12, p=1;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=12, p=2;, score=0.492 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=12, p=2;, score=0.530 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END ...............n_neighbors=12, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=12, p=2;, score=0.547 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=12, p=2;, score=0.547 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=13, p=1;, score=0.503 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=13, p=1;, score=0.526 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=13, p=1;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=13, p=1;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=13, p=1;, score=0.554 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=13, p=2;, score=0.502 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=13, p=2;, score=0.526 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=13, p=2;, score=0.532 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=13, p=2;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=13, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=14, p=1;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=14, p=1;, score=0.529 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=14, p=1;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=14, p=1;, score=0.549 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=14, p=1;, score=0.554 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=14, p=2;, score=0.502 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=14, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=14, p=2;, score=0.535 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=14, p=2;, score=0.548 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=14, p=2;, score=0.551 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=15, p=1;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=15, p=1;, score=0.529 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=15, p=1;, score=0.524 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=15, p=1;, score=0.551 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=15, p=1;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=15, p=2;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=15, p=2;, score=0.520 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=15, p=2;, score=0.531 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=15, p=2;, score=0.544 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=15, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=16, p=1;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=16, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=16, p=1;, score=0.525 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=16, p=1;, score=0.549 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=16, p=1;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=16, p=2;, score=0.509 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=16, p=2;, score=0.519 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=16, p=2;, score=0.533 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=16, p=2;, score=0.548 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=16, p=2;, score=0.546 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=17, p=1;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=17, p=1;, score=0.530 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=17, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=17, p=1;, score=0.552 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=17, p=1;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=17, p=2;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=17, p=2;, score=0.518 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=17, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=17, p=2;, score=0.550 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=17, p=2;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=18, p=1;, score=0.501 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=18, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=18, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=18, p=1;, score=0.552 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=18, p=1;, score=0.551 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=18, p=2;, score=0.512 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=18, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=18, p=2;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=18, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=18, p=2;, score=0.553 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=19, p=1;, score=0.501 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=19, p=1;, score=0.529 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=19, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=19, p=1;, score=0.550 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=19, p=1;, score=0.558 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=19, p=2;, score=0.510 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=19, p=2;, score=0.526 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=19, p=2;, score=0.533 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=19, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=19, p=2;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=20, p=1;, score=0.501 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=20, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=20, p=1;, score=0.532 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=20, p=1;, score=0.547 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=20, p=1;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=20, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=20, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=20, p=2;, score=0.532 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=20, p=2;, score=0.550 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=20, p=2;, score=0.553 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=21, p=1;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=21, p=1;, score=0.531 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=21, p=1;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=21, p=1;, score=0.546 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=21, p=1;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=21, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=21, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=21, p=2;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=21, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=21, p=2;, score=0.555 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=22, p=1;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=22, p=1;, score=0.532 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=22, p=1;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=22, p=1;, score=0.544 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=22, p=1;, score=0.554 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=22, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=22, p=2;, score=0.522 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=22, p=2;, score=0.532 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=22, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=22, p=2;, score=0.555 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=23, p=1;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=23, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=23, p=1;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=23, p=1;, score=0.543 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=23, p=1;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=23, p=2;, score=0.505 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=23, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=23, p=2;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=23, p=2;, score=0.545 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=23, p=2;, score=0.558 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=24, p=1;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=24, p=1;, score=0.532 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=24, p=1;, score=0.531 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=24, p=1;, score=0.542 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ...............n_neighbors=24, p=1;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=24, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=24, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=24, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=24, p=2;, score=0.544 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=24, p=2;, score=0.559 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=25, p=1;, score=0.509 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=25, p=1;, score=0.532 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=25, p=1;, score=0.531 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=25, p=1;, score=0.540 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=25, p=1;, score=0.555 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=25, p=2;, score=0.505 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=25, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=25, p=2;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=25, p=2;, score=0.546 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=25, p=2;, score=0.558 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=26, p=1;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=26, p=1;, score=0.530 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=26, p=1;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=26, p=1;, score=0.541 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=26, p=1;, score=0.552 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=26, p=2;, score=0.503 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=26, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=26, p=2;, score=0.533 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=26, p=2;, score=0.544 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=26, p=2;, score=0.559 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=27, p=1;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=27, p=1;, score=0.532 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=27, p=1;, score=0.529 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=27, p=1;, score=0.540 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=27, p=1;, score=0.551 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=27, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=27, p=2;, score=0.527 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=27, p=2;, score=0.534 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=27, p=2;, score=0.543 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=27, p=2;, score=0.559 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=28, p=1;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=28, p=1;, score=0.533 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=28, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=28, p=1;, score=0.541 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=28, p=1;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=28, p=2;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=28, p=2;, score=0.527 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=28, p=2;, score=0.531 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=28, p=2;, score=0.543 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=28, p=2;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=29, p=1;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=29, p=1;, score=0.530 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=29, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=29, p=1;, score=0.540 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=29, p=1;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=29, p=2;, score=0.511 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=29, p=2;, score=0.527 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=29, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=29, p=2;, score=0.542 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=29, p=2;, score=0.557 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=30, p=1;, score=0.509 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=30, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=30, p=1;, score=0.526 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=30, p=1;, score=0.539 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=30, p=1;, score=0.547 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=30, p=2;, score=0.512 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=30, p=2;, score=0.528 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=30, p=2;, score=0.528 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=30, p=2;, score=0.542 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=30, p=2;, score=0.557 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=31, p=1;, score=0.510 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=31, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=31, p=1;, score=0.523 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=31, p=1;, score=0.537 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=31, p=1;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=31, p=2;, score=0.513 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=31, p=2;, score=0.526 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=31, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=31, p=2;, score=0.538 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=31, p=2;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=32, p=1;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=32, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=32, p=1;, score=0.523 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=32, p=1;, score=0.535 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=32, p=1;, score=0.547 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=32, p=2;, score=0.510 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=32, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=32, p=2;, score=0.526 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=32, p=2;, score=0.537 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=32, p=2;, score=0.554 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=33, p=1;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=33, p=1;, score=0.525 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=33, p=1;, score=0.520 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=33, p=1;, score=0.535 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=33, p=1;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=33, p=2;, score=0.509 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=33, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=33, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=33, p=2;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=33, p=2;, score=0.552 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=34, p=1;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=34, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=34, p=1;, score=0.519 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=34, p=1;, score=0.536 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=34, p=1;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=34, p=2;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=34, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=34, p=2;, score=0.523 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=34, p=2;, score=0.537 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=34, p=2;, score=0.552 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ...............n_neighbors=35, p=1;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=35, p=1;, score=0.526 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=35, p=1;, score=0.517 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=35, p=1;, score=0.535 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=35, p=1;, score=0.547 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=35, p=2;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=35, p=2;, score=0.522 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=35, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=35, p=2;, score=0.535 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=35, p=2;, score=0.554 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=36, p=1;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=36, p=1;, score=0.524 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=36, p=1;, score=0.517 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=36, p=1;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=36, p=1;, score=0.546 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=36, p=2;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=36, p=2;, score=0.521 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=36, p=2;, score=0.522 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=36, p=2;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=36, p=2;, score=0.554 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=37, p=1;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=37, p=1;, score=0.523 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=37, p=1;, score=0.518 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=37, p=1;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=37, p=1;, score=0.546 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=37, p=2;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=37, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=37, p=2;, score=0.524 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=37, p=2;, score=0.532 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=37, p=2;, score=0.552 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=38, p=1;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=38, p=1;, score=0.521 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=38, p=1;, score=0.518 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=38, p=1;, score=0.530 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=38, p=1;, score=0.545 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=38, p=2;, score=0.510 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=38, p=2;, score=0.522 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=38, p=2;, score=0.522 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=38, p=2;, score=0.532 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=38, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=39, p=1;, score=0.502 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=39, p=1;, score=0.520 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=39, p=1;, score=0.516 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=39, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=39, p=1;, score=0.544 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=39, p=2;, score=0.508 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=39, p=2;, score=0.520 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=39, p=2;, score=0.520 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=39, p=2;, score=0.532 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=39, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=40, p=1;, score=0.501 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=40, p=1;, score=0.519 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=40, p=1;, score=0.515 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=40, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=40, p=1;, score=0.544 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=40, p=2;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=40, p=2;, score=0.519 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=40, p=2;, score=0.517 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=40, p=2;, score=0.532 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=40, p=2;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=41, p=1;, score=0.501 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=41, p=1;, score=0.518 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=41, p=1;, score=0.515 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=41, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=41, p=1;, score=0.544 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=41, p=2;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=41, p=2;, score=0.519 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=41, p=2;, score=0.515 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=41, p=2;, score=0.533 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=41, p=2;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=42, p=1;, score=0.501 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=42, p=1;, score=0.517 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=42, p=1;, score=0.515 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=42, p=1;, score=0.526 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=42, p=1;, score=0.542 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=42, p=2;, score=0.507 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=42, p=2;, score=0.518 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=42, p=2;, score=0.516 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=42, p=2;, score=0.531 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=42, p=2;, score=0.549 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=43, p=1;, score=0.502 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=43, p=1;, score=0.516 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=43, p=1;, score=0.513 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=43, p=1;, score=0.528 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=43, p=1;, score=0.543 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=43, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=43, p=2;, score=0.518 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=43, p=2;, score=0.515 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=43, p=2;, score=0.529 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=43, p=2;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=44, p=1;, score=0.502 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=44, p=1;, score=0.514 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=44, p=1;, score=0.512 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=44, p=1;, score=0.526 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=44, p=1;, score=0.542 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=44, p=2;, score=0.506 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=44, p=2;, score=0.517 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=44, p=2;, score=0.513 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=44, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=44, p=2;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=45, p=1;, score=0.502 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=45, p=1;, score=0.512 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=45, p=1;, score=0.511 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=45, p=1;, score=0.527 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=45, p=1;, score=0.543 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=45, p=2;, score=0.505 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=45, p=2;, score=0.518 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=45, p=2;, score=0.513 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=45, p=2;, score=0.529 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=45, p=2;, score=0.546 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=46, p=1;, score=0.501 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ...............n_neighbors=46, p=1;, score=0.509 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=46, p=1;, score=0.509 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=46, p=1;, score=0.525 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=46, p=1;, score=0.541 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=46, p=2;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=46, p=2;, score=0.517 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=46, p=2;, score=0.511 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=46, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=46, p=2;, score=0.546 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=47, p=1;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=47, p=1;, score=0.509 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=47, p=1;, score=0.508 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=47, p=1;, score=0.525 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=47, p=1;, score=0.540 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=47, p=2;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=47, p=2;, score=0.516 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=47, p=2;, score=0.510 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=47, p=2;, score=0.530 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=47, p=2;, score=0.544 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=48, p=1;, score=0.499 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=48, p=1;, score=0.510 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=48, p=1;, score=0.508 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=48, p=1;, score=0.524 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=48, p=1;, score=0.539 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=48, p=2;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=48, p=2;, score=0.515 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=48, p=2;, score=0.510 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=48, p=2;, score=0.531 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=48, p=2;, score=0.542 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=49, p=1;, score=0.501 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=49, p=1;, score=0.509 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=49, p=1;, score=0.507 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=49, p=1;, score=0.524 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=49, p=1;, score=0.538 total time=   0.0s\n",
      "[CV 1/5] END ...............n_neighbors=49, p=2;, score=0.504 total time=   0.0s\n",
      "[CV 2/5] END ...............n_neighbors=49, p=2;, score=0.515 total time=   0.0s\n",
      "[CV 3/5] END ...............n_neighbors=49, p=2;, score=0.509 total time=   0.0s\n",
      "[CV 4/5] END ...............n_neighbors=49, p=2;, score=0.531 total time=   0.0s\n",
      "[CV 5/5] END ...............n_neighbors=49, p=2;, score=0.542 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=KNeighborsRegressor(),\n",
       "             param_grid={'n_neighbors': range(1, 50), 'p': [1, 2]}, verbose=3)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params_grid = {\"n_neighbors\": range(1, 50), \"p\": [1,2]}\n",
    "grid = GridSearchCV(KNeighborsRegressor(), params_grid,refit=True,verbose=3)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 19, 'p': 2}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  2.1880618850148883\n"
     ]
    }
   ],
   "source": [
    "test_preds_grid = grid.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds_grid)\n",
    "\n",
    "print('Test RMSE: ', sqrt(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
